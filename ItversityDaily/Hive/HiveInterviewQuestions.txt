CHallange1:
     One of the Hive's biggest problem used to be skewed key's on joins.
     The way Hive is implimented, a join distributes different values of the key being joined
     across reducers such that a unique value is joined on a single reducer.

Table 1:
user_id  name
1 markzuckerberg
2 johndoe

Table 2:
user_id follower_id
2 34
2 55
1 2
1 3
1 6
1 6
... (millions of people following markzuckerberg)

Then if you join the two into a single table of the schema

user_id name follower_id

On the user_id column, then everything with user_id = 1 passes through a single reducer. 
That means that even if the John Does of the world join fairly quickly, you'll still blocked on the skewed keys 
for the final table to land.
Reference: https://www.quora.com/What-production-issues-have-people-faced-with-Hive-PIG-or-Sqoop
===========================================Challange 2===============================
while working on batch migration project we developed our scripts for the small datasets i.e. sample data in 
our development cluster. you designed your algorithm by handling all the scenarios that can occur in sample data 
but the moment your logic ran over million of records there might be a chance of few scenarios which you dint handle
and resulting into your job failure. This runtime error could be some garbage value or may be some special character 
anything could make your job vulnerable. Advice Never rely on sample data for developing your logic or model. before 
sending it to validation always run it with actual data.
example: The data sample data set is tab delimited with 12 columns. When i run the algorithm in production
the code broken because on of the record has 13 columns. The DelearName column ia having one string in sample records
but for one record the name is Michiel james with tab delimited in between.
====================================================
Partitioning and Bucketing in HIve:
Partition:
Partitioning of table data is done for distributing load horizantally.
bucketing works well when the field has high cardinality and data is evenly distributed among buckets. 
Partitioning works best when the cardinality of the partitioning field is not too high.
Effective for low volume data for a given partition. But some queries like group by on high volume of data 
still take long time to execute. e.g. Grouping of population of China will take long time compared to grouping of population
 in Vatican city. Partition is not solving responsiveness problem in case of data skewing towards a particular partition value.
 Reference:
 https://stackoverflow.com/questions/19128940/what-is-the-difference-between-partitioning-and-bucketing-a-table-in-hive
==============5 ways to increase the performance of Hive queries==============
1.set hive.execution.engine=tez;  insted of mapreduce.
2. Use ORC File.
3. Use Vectorization:
Vectorized query execution improves performance of operations like scans, aggregations, filters and joins, 
by performing them in batches of 1024 rows at once instead of single row each time.
set hive.vectorized.execution.enabled = true;
set hive.vectorized.execution.reduce.enabled = true;
4.Good Sql
=================

HIVE:
Sampling in Hive.
Serde hive
partitioning and bucketing in Hive.
external and managed tables.

Sqoop:
Hbase:
Flume version: 1.6.0
Oozie Version: 